{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTzYGvNXxpkJLKkVNm24uc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroLauton/projeto_imersao_alura/blob/main/Projeto_imersao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHcIAFf7Cqru",
        "outputId": "6e2c7ded-2453-4c0d-8180-b157ce7feb9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------\n",
            "\n",
            "Olá! Eu sou o seu assistente virtual, no que posso ajudar?\n",
            "\n",
            "Menu:\n",
            "\n",
            "1. Realizar uma pesquisa.\n",
            "2. Consultar pesquisas salvas.\n",
            "3. Sair\n",
            "\n",
            "------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Define a API KEY\n",
        "GOOGLE_API_KEY = \"SUA_API_KEY\"  # Substitua 'SUA_API_KEY' pela sua chave de API real\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Define as configurações de geração e segurança\n",
        "generation_config = {\"candidate_count\": 1, \"temperature\": 0.5}\n",
        "safety_settings = {\n",
        "    \"HATE\": \"BLOCK_NONE\",\n",
        "    \"HARASSMENT\": \"BLOCK_NONE\",\n",
        "    \"SEXUAL\": \"BLOCK_NONE\",\n",
        "    \"DANGEROUS\": \"BLOCK_NONE\",\n",
        "}\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.0-pro\",\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings,\n",
        ")\n",
        "\n",
        "# Lista para armazenar pesquisas salvas (pergunta, resposta, historico)\n",
        "pesquisas_salvas = []\n",
        "\n",
        "# Função para salvar uma pesquisa (sem embeddings)\n",
        "def salvar_pesquisa(pergunta, resposta):\n",
        "    pesquisas_salvas.append((pergunta, resposta, []))  # Inicializa histórico vazio\n",
        "    print(\"\\nPesquisa salva com sucesso!\\n\")\n",
        "\n",
        "# Função para exibir pesquisas salvas e permitir a seleção\n",
        "def exibir_pesquisas_salvas():\n",
        "    if pesquisas_salvas:\n",
        "        print(\"\\n------------------------------------------------------------------\")\n",
        "        print(\"\\nPesquisas salvas:\\n\")\n",
        "        for i, (pergunta, resposta, _) in enumerate(pesquisas_salvas):\n",
        "            print(f\"{i + 1}. {pergunta}\")\n",
        "            print(f\"   Trecho: {resposta[:100]}...\")  # Exibe um trecho da resposta\n",
        "        while True:\n",
        "            try:\n",
        "                selecao = int(input(\"\\nSelecione uma pesquisa (ou 0 para voltar): \"))\n",
        "                if selecao == 0:\n",
        "                    break\n",
        "                elif 1 <= selecao <= len(pesquisas_salvas):\n",
        "                    pergunta_selecionada, resposta_selecionada, historico = pesquisas_salvas[\n",
        "                        selecao - 1\n",
        "                    ]\n",
        "                    print(\"\\n------------------------------------------------------------------\")\n",
        "                    print(f\"\\nPergunta selecionada: {pergunta_selecionada}\")\n",
        "                    print(f\"Resposta: {resposta_selecionada}\\n\")\n",
        "                    # Exibe o histórico de perguntas/respostas\n",
        "                    if historico:\n",
        "                        print(\"Histórico:\")\n",
        "                        for pergunta, resposta in historico:\n",
        "                            print(f\"  P: {pergunta}\")\n",
        "                            print(f\"  R: {resposta}\\n\")\n",
        "                else:\n",
        "                    print(\"Seleção inválida. Tente novamente.\")\n",
        "            except ValueError:\n",
        "                print(\"Entrada inválida. Digite um número.\")\n",
        "    else:\n",
        "        print(\"\\nNenhuma pesquisa salva.\")\n",
        "\n",
        "# Função para exibir o menu de opções\n",
        "def exibir_menu():\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    print(\"\\nOlá! Eu sou o seu assistente virtual, no que posso ajudar?\")\n",
        "    print(\"\\nMenu:\\n\")\n",
        "    print(\"1. Realizar uma pesquisa.\")\n",
        "    print(\"2. Consultar pesquisas salvas.\")\n",
        "    print(\"3. Sair\\n\")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "\n",
        "# Loop principal do programa\n",
        "while True:\n",
        "    # Exibe o menu de opções\n",
        "    exibir_menu()\n",
        "    # Obtém a opção escolhida pelo usuário\n",
        "    opcao = input(\"\\nEscolha uma opção: \")\n",
        "    # Processa a opção escolhida\n",
        "    if opcao == '1' or opcao == 'realizar uma pesquisa' or opcao == 'Realizar uma pesquisa':\n",
        "        # Inicia um chat com o modelo de linguagem\n",
        "        chat = model.start_chat(history=[])\n",
        "        # Obtém o prompt do usuário\n",
        "        print(\"\\n------------------------------------------------------------------\")\n",
        "        prompt = input('\\n\\nComando: ')\n",
        "        # Envia o prompt para o modelo de linguagem e obtém a resposta\n",
        "        response = chat.send_message(prompt)\n",
        "        # Imprime a resposta do modelo de linguagem\n",
        "        print(\"\\nResposta:\", response.text, '\\n\\n')\n",
        "        # Solicita ao usuário para realizar uma nova pesquisa ou salvar a pesquisa atual\n",
        "        print(\"------------------------------------------------------------------\\n\")\n",
        "        print(\"1. Realizar nova pesquisa?    |    2. Salvar pesquisa!\")\n",
        "        escolha = input('Decisão: ')\n",
        "        if escolha == '2' or escolha == \"Salvar pesquisa\" or escolha == \"salvar pesquisa\":\n",
        "            salvar_pesquisa(prompt, response.text)  # Salva a pesquisa atual\n",
        "    elif opcao == '2':\n",
        "        exibir_pesquisas_salvas()  # Exibe as pesquisas salvas\n",
        "        input(\"Pressione Enter para voltar ao menu...\")\n",
        "    elif opcao == '3':\n",
        "        # Exibe a mensagem de saída\n",
        "        print(\"Saindo...\")\n",
        "        # Sai do loop principal do programa\n",
        "        break\n",
        "    else:\n",
        "        # Exibe uma mensagem de erro caso a opção seja inválida\n",
        "        print(\"\\nOpção inválida. Tente novamente.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JYcFMC27Zevd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}